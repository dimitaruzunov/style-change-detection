{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "def get_data(folder_name):\n",
    "    x = []\n",
    "    y = []\n",
    "    positions = []\n",
    "    file_names = []\n",
    "\n",
    "    for file in os.listdir(folder_name):\n",
    "        if file.endswith(\".txt\"):\n",
    "            file_name = os.path.join(folder_name, file[:-4])\n",
    "\n",
    "            file_text = open(file_name + '.txt', encoding='utf8')\n",
    "            try:\n",
    "                file_truth = open(file_name + '.truth', encoding='utf8')\n",
    "\n",
    "                try:\n",
    "                    text = file_text.read()\n",
    "                    truth = json.load(file_truth)\n",
    "                    truth_changes = truth['changes']\n",
    "                    truth_positions = truth['positions']\n",
    "\n",
    "                    x.append(text)\n",
    "                    y.append(truth_changes)\n",
    "                    positions.append(truth_positions)\n",
    "                    file_names.append(file[:-4])\n",
    "                finally:\n",
    "                    file_truth.close()\n",
    "            finally:\n",
    "                file_text.close()\n",
    "\n",
    "    return x, y, positions, file_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_DIR = '../../data/training'\n",
    "X, y, positions, file_names = get_data(TRAINING_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "\n",
    "top_words = 10000\n",
    "\n",
    "tokenizer = Tokenizer(lower=True, split=\" \")\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_doc_length = 2000\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_doc_length)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_doc_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# load the whole embedding into memory\n",
    "embeddings_index = dict()\n",
    "f = open('../../data/external/embeddings/glove.6B.100d.txt', encoding=\"utf8\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a weight matrix for words in training docs\n",
    "embedding_matrix = np.zeros((vocab_size, 100))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 2000, 100)         4612200   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 4,692,701\n",
      "Trainable params: 4,692,701\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 1996 samples, validate on 984 samples\n",
      "Epoch 1/15\n",
      "1996/1996 [==============================] - 121s 60ms/step - loss: 0.7026 - acc: 0.5020 - val_loss: 0.6943 - val_acc: 0.5020\n",
      "Epoch 2/15\n",
      "1996/1996 [==============================] - 147s 73ms/step - loss: 0.6928 - acc: 0.5356 - val_loss: 0.6925 - val_acc: 0.5203\n",
      "Epoch 3/15\n",
      "1996/1996 [==============================] - 166s 83ms/step - loss: 0.6875 - acc: 0.5411 - val_loss: 0.7009 - val_acc: 0.5000\n",
      "Epoch 4/15\n",
      "1996/1996 [==============================] - 171s 86ms/step - loss: 0.6804 - acc: 0.5807 - val_loss: 0.6950 - val_acc: 0.5102\n",
      "Epoch 5/15\n",
      "1996/1996 [==============================] - 173s 87ms/step - loss: 0.6664 - acc: 0.6122 - val_loss: 0.6943 - val_acc: 0.5437\n",
      "Epoch 6/15\n",
      "1996/1996 [==============================] - 185s 93ms/step - loss: 0.6528 - acc: 0.6142 - val_loss: 0.6986 - val_acc: 0.5315\n",
      "Epoch 7/15\n",
      "1996/1996 [==============================] - 179s 90ms/step - loss: 0.6323 - acc: 0.6558 - val_loss: 0.7006 - val_acc: 0.5346\n",
      "Epoch 8/15\n",
      "1996/1996 [==============================] - 200s 100ms/step - loss: 0.5990 - acc: 0.6944 - val_loss: 0.7121 - val_acc: 0.5366\n",
      "Epoch 9/15\n",
      "1996/1996 [==============================] - 204s 102ms/step - loss: 0.5456 - acc: 0.7270 - val_loss: 0.7834 - val_acc: 0.5264\n",
      "Epoch 10/15\n",
      "1996/1996 [==============================] - 191s 96ms/step - loss: 0.5000 - acc: 0.7590 - val_loss: 0.8155 - val_acc: 0.5325\n",
      "Epoch 11/15\n",
      "1996/1996 [==============================] - 220s 110ms/step - loss: 0.4263 - acc: 0.8031 - val_loss: 0.8750 - val_acc: 0.5315\n",
      "Epoch 12/15\n",
      "1996/1996 [==============================] - 217s 109ms/step - loss: 0.3487 - acc: 0.8517 - val_loss: 0.9100 - val_acc: 0.5183\n",
      "Epoch 13/15\n",
      "1996/1996 [==============================] - 209s 105ms/step - loss: 0.2896 - acc: 0.8803 - val_loss: 1.0366 - val_acc: 0.5152\n",
      "Epoch 14/15\n",
      "1996/1996 [==============================] - 217s 109ms/step - loss: 0.2232 - acc: 0.9188 - val_loss: 1.1576 - val_acc: 0.5183\n",
      "Epoch 15/15\n",
      "1996/1996 [==============================] - 240s 120ms/step - loss: 0.1761 - acc: 0.9329 - val_loss: 1.1970 - val_acc: 0.5163\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2a8208e0940>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=max_doc_length))\n",
    "model.add(LSTM(100, dropout=0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=15, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 51.63%\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
