{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "def get_data(folder_name):\n",
    "    x = []\n",
    "    y = []\n",
    "    positions = []\n",
    "    file_names = []\n",
    "\n",
    "    for file in os.listdir(folder_name):\n",
    "        if file.endswith(\".txt\"):\n",
    "            file_name = os.path.join(folder_name, file[:-4])\n",
    "\n",
    "            file_text = open(file_name + '.txt', encoding='utf8')\n",
    "            try:\n",
    "                file_truth = open(file_name + '.truth', encoding='utf8')\n",
    "\n",
    "                try:\n",
    "                    text = file_text.read()\n",
    "                    truth = json.load(file_truth)\n",
    "                    truth_changes = truth['changes']\n",
    "                    truth_positions = truth['positions']\n",
    "\n",
    "                    x.append(text)\n",
    "                    y.append(truth_changes)\n",
    "                    positions.append(truth_positions)\n",
    "                    file_names.append(file[:-4])\n",
    "                finally:\n",
    "                    file_truth.close()\n",
    "            finally:\n",
    "                file_text.close()\n",
    "\n",
    "    return x, y, positions, file_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_DIR = '../data/training'\n",
    "X, y, positions, file_names = get_data(TRAINING_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "from nltk.corpus import words as corpus_words\n",
    "\n",
    "URL_TOKEN = \"_URL_\"\n",
    "NUMBER_TOKEN = \"_LONG_NUM_\"\n",
    "CHAR_SEQUENCE_TOKEN = \"_CHAR_SEQ_\"\n",
    "FILE_PATH_TOKEN = \"_FILE_PATH_\"\n",
    "TRANSLITERATION_TOKEN = \"_TRANSLITERATION_\"\n",
    "WORD_SPLIT_TOKEN = \"_WORD_SPLIT_\"\n",
    "LONG_WORD_TOKEN = \"_LONG_WORD_\"\n",
    "\n",
    "def contains_alnum(word):\n",
    "    for character in word:\n",
    "        if character.isalnum():\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "class BasicPreprocessor():\n",
    "\n",
    "    def __init__(self):\n",
    "        self.params = {\n",
    "            \"replace_long_numbers\" : True,\n",
    "            \"long_word_threshold\" : 50,\n",
    "            \"replace_long_char_sequences\" : True,\n",
    "            \"replace_file_paths\" : True,\n",
    "            \"try_split_words\" : True,\n",
    "            \"add_split_token\" : False,\n",
    "        }\n",
    "        self.url_regex = re.compile('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "        # At least five digits are considered long - 4 digits can be a year, which might be interesting on its own\n",
    "        self.number_regex = re.compile(\"\\d{5,}\")\n",
    "        self.unix_path_regex = re.compile('^(?:/[^/]*)*$')\n",
    "        self.windows_path_regex = re.compile('^(?:[a-zA-Z]\\:|\\\\\\\\[\\w\\.]+\\\\[\\w.$]+)\\\\(?:[\\w]+\\\\)*\\w([\\w.])+$')\n",
    "        self.words = corpus_words.words()\n",
    "\n",
    "    '''\n",
    "        This function is to be called before chunking.\n",
    "        It does some basic processing on the text as\n",
    "        a whole.\n",
    "    '''\n",
    "    def process_text(self, text):\n",
    "        # Perform URL replacement\n",
    "        text = self.url_regex.sub(URL_TOKEN, text)\n",
    "        # Replace long numbers with tag if specified\n",
    "        if self.params[\"replace_long_numbers\"]:\n",
    "            text = self.number_regex.sub(NUMBER_TOKEN, text)\n",
    "        return text\n",
    "\n",
    "    '''\n",
    "        This function is to be called after tokenization.\n",
    "        It goes through the token stream and does some\n",
    "        filtering, replacement, token addition etc\n",
    "    '''\n",
    "    def process_word_list(self, word_list):\n",
    "        output_words = []\n",
    "        for word_candidate in word_list:\n",
    "            word_len = len(word_candidate)\n",
    "            # Sequence of three characters is ok, like ...\n",
    "            if word_len < 4:\n",
    "                output_words.append(word_candidate)\n",
    "                continue\n",
    "            # Check for long sequences of characters (no letters or digits) and replace them with tag if specified\n",
    "            if self.params[\"replace_long_char_sequences\"] and not contains_alnum(word_candidate):\n",
    "                output_words.append(CHAR_SEQUENCE_TOKEN)\n",
    "                continue\n",
    "            if word_len < 10:\n",
    "                output_words.append(word_candidate)\n",
    "                continue\n",
    "            # For words longer than 10 chars check if they are file paths or transliterations\n",
    "            if self.params[\"replace_file_paths\"]:\n",
    "                if self.unix_path_regex.match(word_candidate):\n",
    "                    # Decide whether it's path or transliteration based on unicode characters\n",
    "                    if all(ord(char) < 128 for char in word_candidate):\n",
    "                        output_words.append(FILE_PATH_TOKEN)\n",
    "                        continue\n",
    "                    else:\n",
    "                        output_words.append(TRANSLITERATION_TOKEN)\n",
    "                        continue\n",
    "                if self.windows_path_regex.match(word_candidate):\n",
    "                    output_words.append(FILE_PATH_TOKEN)\n",
    "                    continue\n",
    "            if word_len < 15:\n",
    "                output_words.append(word_candidate)\n",
    "                continue\n",
    "            # For words longer than 15 chars try to split them into more than two pieces\n",
    "            if self.params[\"try_split_words\"]:\n",
    "                word_parts = self.try_split_word(word_candidate)\n",
    "                if len(word_parts) > 2:\n",
    "                    if self.params[\"add_split_token\"]:\n",
    "                        output_words.append(WORD_SPLIT_TOKEN)\n",
    "                    for part in word_parts:\n",
    "                        output_words.append(part)\n",
    "                    continue\n",
    "            # For super long words replace them with token if requested\n",
    "            threshold = self.params[\"long_word_threshold\"]\n",
    "            if threshold > 0 and word_len > threshold:\n",
    "                output_words.append(LONG_WORD_TOKEN)\n",
    "            else:\n",
    "                output_words.append(word_candidate)\n",
    "\n",
    "        return output_words\n",
    "\n",
    "    def try_split_word(self, word):\n",
    "        candidates = word.split('-')\n",
    "        length = len(candidates)\n",
    "        num_in_dict = 0\n",
    "        for candidate in candidates:\n",
    "            if candidate in self.words:\n",
    "                num_in_dict = num_in_dict + 1\n",
    "        if num_in_dict >= length / 2:\n",
    "            return candidates\n",
    "        else:\n",
    "            return [word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "preprocessor = BasicPreprocessor()\n",
    "\n",
    "def get_segments_merge_last(text, n, chunks, wordFilter=None, process=False):\n",
    "    segments = []\n",
    "    words = word_tokenize(text)\n",
    "    if process:\n",
    "        words = preprocessor.process_word_list(words)\n",
    "    x = len(words)\n",
    "    if chunks:\n",
    "        n = round(x / chunks)\n",
    "    n = min(n, x)\n",
    "    i = 0\n",
    "    for i in range(0, x-x%n-n, n):\n",
    "        segments.append(' '.join(words[i:i+n]))\n",
    "    segments.append(' '.join(words[i+n:]))\n",
    "    if wordFilter:\n",
    "        segments = [wordFilter(s) for s in segments]\n",
    "    return segments\n",
    "\n",
    "def get_sliding_words(text, n, chunks=None, wordFilter=None, process=False):\n",
    "    segments = []\n",
    "    words = word_tokenize(text)\n",
    "    if process:\n",
    "        words = preprocessor.process_word_list(words)\n",
    "    x = len(words)\n",
    "    if chunks:\n",
    "        n = round(x / chunks)\n",
    "    n = min(n, x)\n",
    "    i = 0\n",
    "    overlap = round(n/2)\n",
    "    for i in range(0, x-n-overlap, overlap):\n",
    "        segments.append(' '.join(words[i:i+n]))\n",
    "    segments.append(' '.join(words[i+overlap:]))\n",
    "    if wordFilter:\n",
    "        segments = [wordFilter(s) for s in segments]\n",
    "    return segments\n",
    "\n",
    "def word_chunks(X, n=100, chunks=None, wordFilter=None, sliding=False, process=False):\n",
    "    if sliding:\n",
    "        print('Sliding word chunks...')\n",
    "        return np.array([get_sliding_words(text, n, chunks, wordFilter, process) for text in X])\n",
    "\n",
    "    print('Word chunks...')\n",
    "    return np.array([get_segments_merge_last(text, n, chunks, wordFilter, process) for text in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import numpy as np\n",
    "\n",
    "def lexical(X, feature_names=[]):\n",
    "    print('Lexical features...')\n",
    "    transformed = []\n",
    "\n",
    "    for doc in X:\n",
    "        segments = []\n",
    "\n",
    "        for entry in doc:\n",
    "            entry_char = list(entry)\n",
    "            entry_word = word_tokenize(entry)\n",
    "            entry_word_tagged = pos_tag(entry_word)\n",
    "            entry_sent = sent_tokenize(entry)\n",
    "\n",
    "            chars, char_features = lexical_chars(entry_char)\n",
    "            words, word_features = lexical_words(entry_word_tagged)\n",
    "            sentences, sentence_features = lexical_sentences(entry_sent)\n",
    "\n",
    "            segments.append(chars + words + sentences)\n",
    "\n",
    "        transformed.append(segments)\n",
    "\n",
    "    feature_names.extend(char_features + word_features + sentence_features)\n",
    "\n",
    "    return np.array(transformed)\n",
    "\n",
    "def lexical_chars(chars):\n",
    "    char_count = len(chars)\n",
    "\n",
    "    char_analysis = {\n",
    "        'semicolon_count': 0,\n",
    "        'colon_count': 0,\n",
    "        'spaces_count': 0,\n",
    "        'apostrophes_count': 0,\n",
    "        'parenthesis_count': 0\n",
    "    }\n",
    "\n",
    "    for char in chars:\n",
    "        if char == ';': char_analysis['semicolon_count'] += 1\n",
    "        if char == ':': char_analysis['colon_count'] += 1\n",
    "        if char == ' ': char_analysis['spaces_count'] += 1\n",
    "        if char == '\\'': char_analysis['apostrophes_count'] += 1\n",
    "        if char == '(': char_analysis['parenthesis_count'] += 1\n",
    "\n",
    "    feature_names = list(char_analysis.keys())\n",
    "    return [char_analysis[key]/char_count for key in feature_names], feature_names\n",
    "\n",
    "def lexical_words(words_tagged):\n",
    "    word_count = len(words_tagged)\n",
    "\n",
    "    word_analysis = {\n",
    "        'pronouns': 0,\n",
    "        'prepositions': 0,\n",
    "        'adjectives': 0,\n",
    "        'adverbs': 0,\n",
    "        'determiners': 0,\n",
    "        'modals': 0,\n",
    "        'nouns': 0,\n",
    "        'personal_pronouns': 0,\n",
    "        'verbs': 0,\n",
    "        'word_len_gte_six': 0,\n",
    "        'word_len_two_and_three': 0,\n",
    "        'total_word_length': 0,\n",
    "        'all_caps': 0,\n",
    "        'capitalized': 0,\n",
    "        'quotes_count': 0\n",
    "    }\n",
    "\n",
    "    for (word, tag) in words_tagged:\n",
    "        if tag in ['PRP']: word_analysis['personal_pronouns'] += 1\n",
    "        if tag.startswith('J'): word_analysis['adjectives'] += 1\n",
    "        if tag.startswith('N'): word_analysis['nouns'] += 1\n",
    "        if tag.startswith('V'): word_analysis['verbs'] += 1\n",
    "        if tag in ['PRP', 'PRP$', 'WP', 'WP$']: word_analysis['pronouns'] += 1\n",
    "        elif tag in ['IN']: word_analysis['prepositions'] += 1\n",
    "        elif tag in ['RB', 'RBR', 'RBS']: word_analysis['adverbs'] += 1\n",
    "        elif tag in ['DT', 'PDT', 'WDT']: word_analysis['determiners'] += 1\n",
    "        elif tag in ['MD']: word_analysis['modals'] += 1\n",
    "        if len(word) >= 6: word_analysis['word_len_gte_six'] += 1\n",
    "        elif len(word) in [2, 3]: word_analysis['word_len_two_and_three'] += 1\n",
    "        word_analysis['total_word_length'] += len(word)\n",
    "        if word.isupper(): word_analysis['all_caps'] += 1\n",
    "        if not word.isupper() and word[0].isupper(): word_analysis['capitalized'] += 1\n",
    "        word_analysis['quotes_count'] += word.count('\"') + word.count('``') + word.count('\\'\\'')\n",
    "\n",
    "\n",
    "    feature_names = list(word_analysis.keys())\n",
    "    return [word_analysis[key]/word_count for key in feature_names], feature_names\n",
    "\n",
    "def lexical_sentences(sentences):\n",
    "    sent_count = len(sentences)\n",
    "\n",
    "    sent_analysis = {\n",
    "        'question_sentences': 0,\n",
    "        'period_sentences': 0,\n",
    "        'exclamation_sentences': 0,\n",
    "        'short_sentences': 0,\n",
    "        'long_sentences': 0,\n",
    "        'sentence_length': 0\n",
    "    }\n",
    "\n",
    "    for sent in sentences:\n",
    "        if sent[len(sent) - 1] == '?': sent_analysis['question_sentences'] += 1\n",
    "        if sent[len(sent) - 1] == '.': sent_analysis['period_sentences'] += 1\n",
    "        if sent[len(sent) - 1] == '!': sent_analysis['exclamation_sentences'] += 1\n",
    "        if len(sent) <= 100: sent_analysis['short_sentences'] += 1\n",
    "        if len(sent) >= 200: sent_analysis['long_sentences'] += 1\n",
    "        sent_analysis['sentence_length'] += len(sent)\n",
    "\n",
    "\n",
    "    feature_names = list(sent_analysis.keys())\n",
    "    return [sent_analysis[key]/sent_count for key in feature_names], feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(X):\n",
    "    feature_names = []\n",
    "\n",
    "    X = [preprocessor.process_text(x) for x in X]\n",
    "\n",
    "    lexical_features = lexical(word_chunks(X, process=True), feature_names)\n",
    "    \n",
    "    return lexical_features, feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word chunks...\n",
      "Lexical features...\n"
     ]
    }
   ],
   "source": [
    "XV, features = pipeline(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2980,)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XV.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(XV, y, test_size=0.33, random_state=42)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_doc_length = 200\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_doc_length)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_doc_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional_12 (Bidirectio (None, 400)               363200    \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 401       \n",
      "=================================================================\n",
      "Total params: 363,601\n",
      "Trainable params: 363,601\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 1996 samples, validate on 984 samples\n",
      "Epoch 1/15\n",
      "1996/1996 [==============================] - 73s 36ms/step - loss: 0.6987 - acc: 0.5055 - val_loss: 0.6856 - val_acc: 0.4980\n",
      "Epoch 2/15\n",
      "1996/1996 [==============================] - 67s 34ms/step - loss: 0.6843 - acc: 0.5706 - val_loss: 0.6721 - val_acc: 0.6240\n",
      "Epoch 3/15\n",
      "1996/1996 [==============================] - 67s 34ms/step - loss: 0.6795 - acc: 0.5787 - val_loss: 0.6667 - val_acc: 0.6077\n",
      "Epoch 4/15\n",
      "1996/1996 [==============================] - 68s 34ms/step - loss: 0.6672 - acc: 0.6022 - val_loss: 0.6647 - val_acc: 0.5579\n",
      "Epoch 5/15\n",
      "1996/1996 [==============================] - 67s 34ms/step - loss: 0.6610 - acc: 0.5937 - val_loss: 0.6440 - val_acc: 0.6341\n",
      "Epoch 6/15\n",
      "1996/1996 [==============================] - 67s 34ms/step - loss: 0.6444 - acc: 0.6298 - val_loss: 0.6328 - val_acc: 0.6484\n",
      "Epoch 7/15\n",
      "1996/1996 [==============================] - 67s 34ms/step - loss: 0.6384 - acc: 0.6323 - val_loss: 0.6355 - val_acc: 0.6423\n",
      "Epoch 8/15\n",
      "1996/1996 [==============================] - 67s 34ms/step - loss: 0.6315 - acc: 0.6383 - val_loss: 0.6508 - val_acc: 0.6138\n",
      "Epoch 9/15\n",
      "1996/1996 [==============================] - 67s 33ms/step - loss: 0.6525 - acc: 0.6107 - val_loss: 0.6514 - val_acc: 0.6514\n",
      "Epoch 10/15\n",
      "1996/1996 [==============================] - 61s 30ms/step - loss: 0.6350 - acc: 0.6418 - val_loss: 0.6365 - val_acc: 0.6362\n",
      "Epoch 11/15\n",
      "1996/1996 [==============================] - 57s 28ms/step - loss: 0.6296 - acc: 0.6373 - val_loss: 0.6394 - val_acc: 0.6514\n",
      "Epoch 12/15\n",
      "1996/1996 [==============================] - 57s 29ms/step - loss: 0.6314 - acc: 0.6363 - val_loss: 0.6386 - val_acc: 0.6352\n",
      "Epoch 13/15\n",
      "1996/1996 [==============================] - 56s 28ms/step - loss: 0.6338 - acc: 0.6378 - val_loss: 0.6323 - val_acc: 0.6453\n",
      "Epoch 14/15\n",
      "1996/1996 [==============================] - 55s 28ms/step - loss: 0.6297 - acc: 0.6418 - val_loss: 0.6409 - val_acc: 0.6463\n",
      "Epoch 15/15\n",
      "1996/1996 [==============================] - 56s 28ms/step - loss: 0.6257 - acc: 0.6448 - val_loss: 0.6304 - val_acc: 0.6362\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1fd9cb589e8>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the model\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(200, dropout=0.2), input_shape=(max_doc_length, len(features))))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=15, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 63.62%\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
